{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make TFRecords from VOC XML & jpgs\n",
    "\n",
    "THIS IS REDUNDANTE with UnderstandingObjectDetectionExample  \n",
    "Use the other notebook for a full understanding\n",
    "\n",
    "This was taken from ssd-dag repo\n",
    "\n",
    "## Prerequitistes\n",
    "### Input\n",
    "1. you have jpeg images\n",
    "2. you have annotations - XML format, VOC Pascal format standard\n",
    "\n",
    "### Output\n",
    "tfrecords_dir needs to have 3 subdirectories\n",
    "/train\n",
    "/val\n",
    "/test\n",
    "\n",
    "this code leverages the standards and templates as much as possible\n",
    "\n",
    "## [OPTIONAL] Testing / Visualizing\n",
    "This notebook also includes testing your tfrecord files by visualization.  Two methods:\n",
    "- matplotlib\n",
    "- (tensorflow/models)  object_detection.utils  (This is the preferred method)\n",
    "Remember - you don't have a Linux desktop so, you can't use a GTK based solution like OpenCV for the display.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import contextlib2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# from matplotlib.patches import Rectangle\n",
    "import matplotlib.patches as patches\n",
    "# This is needed since we cloned tensorflow/models under code.\n",
    "# - if you don't know what this means\n",
    "#   Look at the notebook TrainModel_Step1_Local\n",
    "#      in this notebook, you basically set up the project with includes cloning \n",
    "#      and compiling the tensorflow/models repo\n",
    "#   we are using the utilities found in that repo\n",
    "\n",
    "cwd = os.getcwd()\n",
    "# this path is different in this project\n",
    "models = os.path.abspath(os.path.join(cwd, '..', 'models/research/'))\n",
    "slim = os.path.abspath(os.path.join(cwd, '..', 'models/research/slim'))\n",
    "sys.path.append(models)\n",
    "sys.path.append(slim)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils.visualization_utils import STANDARD_COLORS\n",
    "from object_detection.utils.visualization_utils import draw_bounding_box_on_image\n",
    "\n",
    "# this is the standard feature dict\n",
    "from example_utils import feature_obj_detect, get_all_tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from example_utils import voc_to_tfrecord_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.15.2\n"
     ]
    }
   ],
   "source": [
    "# this won't work with Tensorflow 2.0\n",
    "# if you have TF 2.0 loaded, you can't set eager - it's forced on\n",
    "\n",
    "print ('TensorFlow Version:', tf.__version__)\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals\n",
    "\n",
    "This has been simplified from the ssd-dag project.   Kinda going back and forth between projects.  But there is no CODE and DATA -- everything is together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "# project directories\n",
    "PROJECT = os.getcwd()\n",
    "HSDATA = '/hsdata'\n",
    "\n",
    "IMAGE_DIR_ROOT = os.path.join(HSDATA, \"jpeg_images\")\n",
    "ANNOTATION_DIR_ROOT = (os.path.join(HSDATA, \"annotation\"))\n",
    "\n",
    "LABEL_MAP_FILE = os.path.join(PROJECT, 'model', 'security_label_map.pbtxt')\n",
    "TFRECORD_DIR = os.path.join(HSDATA, 'tfrecord')\n",
    "TRAINING_SPLIT_TUPLE =  (69,30,1)\n",
    "INCLUDE_CLASSES = 'all'\n",
    "EXCLUDE_TRUNCATED = False,\n",
    "EXCLUDE_DIFFICULT = False\n",
    "\n",
    "SSD_PROJECT = os.path.abspath(os.path.join(cwd, '..', 'ssd-dag'))\n",
    "SSD_TFRECORDS = os.path.join(SSD_PROJECT, 'code/tfrecords')\n",
    "\n",
    "DATA_DATE = '20200505'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move snapshot -> /hsdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls {ANNOTATION_DIR}/*.xml | wc\n",
    "! ls {IMAGE_DIR}/*.jpg | wc\n",
    "! mv snapshot/*.xml {ANNOTATION_DIR}\n",
    "! mv snapshot/*.jpg {IMAGE_DIR}\n",
    "! ls {ANNOTATION_DIR}/*.xml | wc\n",
    "! ls {IMAGE_DIR}/*.jpg | wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve snapshot tarball from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's available\n",
    "! aws s3 ls s3://jmduff.security-system/training_data/ --profile=jmduff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer to ~/Downloads/snapshot\n",
    "snapshot_file = 'snapshot_{}_8100.tar.gz'.format(DATA_DATE)\n",
    "! aws s3 cp s3://jmduff.security-system/training_data/{snapshot_file} ~/Downloads --profile=jmduff\n",
    "! rm -rf ~/Downloads/snapshot\n",
    "! tar -xvf ~/Downloads/{snapshot_file} -C ~/Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to High Speed Data\n",
    "! mv ~/Downloads/snapshot/*.xml {ANNOTATION_DIR}\n",
    "! mv ~/Downloads/snapshot/*.jpg {IMAGE_DIR}\n",
    "\n",
    "# Verify\n",
    "! ls ~/Downloads/snapshot\n",
    "! ls {ANNOTATION_DIR}| wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating .tfrecord files from XML annotations & jpeg images\n",
    "\n",
    "## Fix Labels\n",
    "\n",
    "if you get an error like:  \n",
    "!!! label map error: 20190625_polySauce_spicyBag_1561494037 smallSauce  skipped\n",
    "\n",
    "This is telling you that the image_id:  20190625_polySauce_spicyBag_1561494037  \n",
    "has a class label:  smallSauce  \n",
    "which is not defined in the label map.  (don't be fooled!  'smallSauce' is the problem, not polySauce in the filename)\n",
    "\n",
    "If there are a few - you could ignore it.   To fix the data locally:\n",
    "1. review the label_map - $ cat code/cfa_prod_label_map.pbtxt;   youll see 7 == cfaSauce, 10 == polySauce, \n",
    "2. you need to change any 'smallSauce' to one of the labels in the label_map; we will choose cfaSauce \n",
    "3. add a sed command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to fix the labels\n",
    "# it's in the other notebook:  UnderstaningObjectDetectionExample\n",
    "\n",
    "# this is the label_map as a dict\n",
    "#    {'smallHotDrink': 2, 'nuggBox': 5, 'sandBag': 6, 'smallFry': 8, \n",
    "#     'largeFry': 9, 'cfaSauce': 7, 'mediumColdDrink': 3, 'sandBox': 4, \n",
    "#   'hand': 1, 'spicyBag': 11, 'polySauce': 10}\n",
    "\n",
    "# you might have to replace some names due to inconsistencies in labeling and the map\n",
    "os.chdir(ANNOTATION_DIR)\n",
    "! pwd\n",
    "! sed -i 's/motorcyclew/motorcycle/g' *.xml\n",
    "! sed -i 's/bagw/bag/g' *.xml\n",
    "! sed -i 's/personww/person/g' *.xml\n",
    "! sed -i 's/stroller /stroller/g' *.xml\n",
    "! sed -i 's/motorcycle\\t /motorcycle/g' *.xml\n",
    "! sed -i 's/ mail/mail/g' *.xml\n",
    "! sed -i 's/umbrella /umbrella/g' *.xml\n",
    "os.chdir(PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## voc_to_tfrecord_file()\n",
    "This program is in code/cfa_utils/example_utils.py    (Reminder - this isn't an example, it is tf.Example - ugh!) \n",
    "\n",
    "This progrm leverages as much of the standard TensorFlow code as possible.  That means:\n",
    "- annotations are based on VOC PASCAL data standards.   There are hundreds of program examples that use this data which is an XML annotation format.\n",
    "- the tf.Example(Feature) format is based on the format used in the MobileNet model.    I lifted it out of the /models code and placed it here where you can import it.  It is important that you have a consistent format through tfrecord generation, training, predictoin.   \n",
    "\n",
    "I used a pattern where I imported the SSD Feature dictionary then copied it to a dict - then used that dict in the serialization.   You'll see that in the program.   The point is, the feature (dict) format is defined only once in one place.    (Look at the code.)  Odd side effect:   It seems that you must define every element of the dict.  If you don't, you'll get an error:  \n",
    "--> 214             features = tf.train.Features(feature=feature)\n",
    "    215 \n",
    "    216             tf_example = tf.train.Example(features=features)\n",
    "\n",
    "TypeError: MergeFrom() takes exactly one argument (3 given)\n",
    "\n",
    "This program will tell you if it skips image/annotations due to bad labels.  (explained above).\n",
    "\n",
    "### Result:\n",
    "\n",
    "This is telling you that 3149 had a 'verified' (XML attribute) annotation and 22 were not verified.   That is normal.  WHen you label (using labelImg for example), you can skip a questionable training image by simply not verifying it.\n",
    "\n",
    "This dict also shows label map, e.g. hand == class_id = 1\n",
    "\n",
    "  verified: 3149   not: 22\n",
    "{'hand': 1, 'smallHotDrink': 2, 'mediumColdDrink': 3, 'sandBox': 4, 'nuggBox': 5, 'sandBag': 6, 'cfaSauce': 7, 'smallFry': 8, 'largeFry': 9, 'polySauce': 10, 'spicyBag': 11}\n",
    "\n",
    "This is telling you 1889 images were written to the train.tfrecord file.  (not sharded)  \n",
    "169 objects were class_id = 6 (sandBag)  \n",
    "568 objects were class_id = 4 (nuggBox)  \n",
    "These totals will sum >= 1889 because there may be multiple objects per image.\n",
    "\n",
    " -- images 1889  writing to: /home/ec2-user/SageMaker/ssd-dag/tmp/train/train.tfrecord\n",
    "     image count: 1889   class_count: {6: 169, 9: 286, 5: 563, 11: 178, 2: 441, 4: 568, 8: 291, 1: 927, 3: 572, 10: 412, 7: 157}\n",
    "     \n",
    "### file output\n",
    "NOTE - these files were written (depending on your GLOBAL value) to /tmp.    Write to tmp, then promote to S3 if you want to use these.    Look at the training program (notebook) to see where it pulls tfrecords (hint: it won't be /tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear out the (*.record-*-of-*) output directory\n",
    "! rm {TFRECORD_DIR}/train/*.*\n",
    "! rm {TFRECORD_DIR}/val/*.*\n",
    "! rm {TFRECORD_DIR}/test/*.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- MAKING IMAGES LISTS - train/val/test\n",
      "   found 12 subdirectories\n",
      "   making list of verified annotations -- verified count 0 / non 0 in /hsdata/annotation/202006\n",
      "   making list of verified annotations -- verified count 10934 / non 0 in /hsdata/annotation/202002\n",
      "   making list of verified annotations -- verified count 11301 / non 0 in /hsdata/annotation/202001\n",
      "   making list of verified annotations -- verified count 0 / non 0 in /hsdata/annotation/202008\n",
      "   making list of verified annotations -- verified count 0 / non 0 in /hsdata/annotation/202011\n",
      "   making list of verified annotations -- verified count 0 / non 0 in /hsdata/annotation/202012\n",
      "   making list of verified annotations -- verified count 18768 / non 0 in /hsdata/annotation/202004\n",
      "   making list of verified annotations -- verified count 0 / non 0 in /hsdata/annotation/202009\n",
      "   making list of verified annotations -- verified count 0 / non 0 in /hsdata/annotation/202010\n",
      "   making list of verified annotations -- verified count 6021 / non 0 in /hsdata/annotation/202005\n",
      "   making list of verified annotations -- verified count 19473 / non 0 in /hsdata/annotation/202003\n",
      "   making list of verified annotations -- verified count 0 / non 0 in /hsdata/annotation/202007\n",
      "-- TOTAL list of verified annotations -- verfied count 66497\n",
      "   split counts: 45882 19949 664\n",
      "   making randomized lists\n",
      "\n",
      "\n",
      "         /hsdata/annotation/202001/1583691199\n",
      "         /hsdata/annotation/202003/1586009261-0-0\n",
      "         /hsdata/annotation/202003/1586272104-0-0\n",
      "         /hsdata/annotation/202004/15874206727-1-0-c\n",
      "         /hsdata/annotation/202002/1585916641-1-1\n",
      "         /hsdata/annotation/202003/15869773842-0-0\n",
      "         /hsdata/annotation/202003/15867121189-0-1\n",
      "         /hsdata/annotation/202004/15872479813-2-1\n",
      "         /hsdata/annotation/202003/1586463796-1-2\n",
      "         /hsdata/annotation/202002/1585262088_0_3\n",
      "\n",
      "\n",
      "         /hsdata/annotation/202004/15874989092-1-0-c\n",
      "         /hsdata/annotation/202004/15872463975-0-1\n",
      "         /hsdata/annotation/202003/15868972544-0-0\n",
      "         /hsdata/annotation/202003/15866296283-0-0\n",
      "         /hsdata/annotation/202002/1585259475_0_3\n",
      "         /hsdata/annotation/202005/15884618922-0-2-c\n",
      "         /hsdata/annotation/202003/15869560050-0-3\n",
      "         /hsdata/annotation/202004/15877440455-0-3-c\n",
      "         /hsdata/annotation/202003/1586383806-1-0\n",
      "         /hsdata/annotation/202004/15879038767-2-1-g\n",
      "\n",
      "\n",
      "         /hsdata/annotation/202002/1585313412_0_0\n",
      "         /hsdata/annotation/202003/1586531539-0-0\n",
      "         /hsdata/annotation/202001/1584101966\n",
      "         /hsdata/annotation/202004/15872464279-0-1\n",
      "         /hsdata/annotation/202001/1583691415\n",
      "         /hsdata/annotation/202003/1586109511-1-0\n",
      "         /hsdata/annotation/202002/1585069716_0_0\n",
      "         /hsdata/annotation/202001/1584105264\n",
      "         /hsdata/annotation/202005/15886797926-2-2-c\n",
      "         /hsdata/annotation/202002/1585769660-1-2\n",
      "{'person': 1, 'bicycle': 2, 'car': 3, 'motorcycle': 4, 'airplane': 5, 'bus': 6, 'train': 7, 'truck': 8, 'boat': 9, 'traffic light': 10, 'fire hydrant': 11, 'stop sign': 13, 'parking meter': 14, 'bench': 15, 'bird': 16, 'cat': 17, 'dog': 18, 'ogddn': 19, 'ogdup': 20, 'igddn': 21, 'igdup': 22, 'scooter': 23, 'package': 24, 'bag': 25, 'backpack': 27, 'umbrella': 28, 'handbag': 31, 'stroller': 32, 'suitcase': 33, 'mail': 34, 'gmark0': 35, 'gmark1': 36, 'sports ball': 37, 'kite': 38, 'baseball bat': 39, 'baseball glove': 40, 'skateboard': 41, 'surfboard': 42, 'tennis racket': 43, 'bottle': 44, 'wine glass': 46, 'cup': 47, 'fork': 48, 'knife': 49, 'spoon': 50, 'bowl': 51, 'banana': 52, 'apple': 53, 'sandwich': 54, 'orange': 55, 'broccoli': 56, 'carrot': 57, 'hot dog': 58, 'pizza': 59, 'donut': 60, 'cake': 61, 'chair': 62, 'couch': 63, 'potted plant': 64, 'bed': 65, 'dining table': 67, 'toilet': 70, 'tv': 72, 'laptop': 73, 'mouse': 74, 'remote': 75, 'keyboard': 76, 'cell phone': 77, 'microwave': 78, 'oven': 79, 'toaster': 80, 'sink': 81, 'refrigerator': 82, 'book': 84, 'clock': 85, 'vase': 86, 'scissors': 87, 'teddy bear': 88, 'hair drier': 89, 'toothbrush': 90}\n",
      " -- images 45882  writing to: /hsdata/tfrecord/train/train.record\n",
      "WARNING:tensorflow:From /home/security/projects/models/research/object_detection/dataset_tools/tf_record_creation_util.py:43: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "!!! label map error: /hsdata/annotation/202001/1584813889_0_2 w  skipped\n",
      "!!! label map error: /hsdata/annotation/202001/1583612784 horse  skipped\n",
      "!!! label map error: /hsdata/annotation/202003/15867931733-1-0 zebra  skipped\n",
      "!!! label map error: /hsdata/annotation/202003/1586186079-1-0 personw  skipped\n",
      "     image count: 45882   class_count: {3: 23754, 1: 31874, 24: 995, 32: 330, 35: 345, 4: 2221, 2: 3976, 27: 455, 18: 1279, 25: 241, 8: 1283, 28: 187, 34: 458, 6: 295, 64: 59, 23: 14, 84: 8, 41: 20, 79: 2, 17: 3}\n",
      " -- images 19949  writing to: /hsdata/tfrecord/val/val.record\n",
      "!!! label map error: /hsdata/annotation/202003/15865487371-1-2 zebra  skipped\n",
      "     image count: 19949   class_count: {1: 13990, 41: 7, 3: 10117, 25: 85, 34: 229, 18: 613, 35: 161, 2: 1678, 24: 456, 4: 992, 6: 123, 28: 82, 8: 534, 32: 151, 27: 196, 79: 2, 64: 29, 17: 7, 23: 5, 84: 4}\n",
      " -- images 666  writing to: /hsdata/tfrecord/test/test.record\n",
      "     image count: 666   class_count: {3: 329, 1: 476, 8: 24, 2: 60, 6: 4, 18: 23, 24: 21, 27: 11, 34: 13, 4: 28, 25: 4, 32: 3, 35: 3, 28: 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_to_tfrecord_file(IMAGE_DIR_ROOT,\n",
    "                    ANNOTATION_DIR_ROOT,\n",
    "                    LABEL_MAP_FILE,\n",
    "                    TFRECORD_DIR,\n",
    "                    TRAINING_SPLIT_TUPLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Test your TFRecords \n",
    "Select your source of tfrecords\n",
    "data/tfrecords is the source used in training.  \n",
    "tmp is the source you just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFRECORD_DIR = '/home/ec2-user/SageMaker/ssd-dag/data/tfrecords'\n",
    "# TFRECORD_DIR = '/home/ec2-user/SageMaker/ssd-dag/tmp'\n",
    "print (TFRECORD_DIR)\n",
    "! ls {TFRECORD_DIR}/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will read a list of files\n",
    "# let's combine train, val & test\n",
    "\n",
    "\n",
    "tfrecord_file_list_input = get_all_tfrecords([os.path.join(TFRECORD_DIR, 'train'),\n",
    "                            os.path.join(TFRECORD_DIR, 'val'),\n",
    "                            os.path.join(TFRECORD_DIR, 'test')])\n",
    "print (\"reading:\", tfrecord_file_list_input)\n",
    "raw_dataset = tf.data.TFRecordDataset(tfrecord_file_list_input)\n",
    "raw_dataset.cache()  # cache to memory\n",
    "raw_dataset.shuffle(buffer_size=5000)\n",
    "print (type(raw_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the TFRecords\n",
    "- The file is read into a dataset (TFRecordDatasetV1 to be exact)\n",
    "- Iterating through the records:\n",
    "  - each record is an EagerTensor (you must have Eager Execution enabled)\n",
    "  - This tensor has a serialized tf.Example\n",
    "      - byte string\n",
    "      - get the value (byte string) with .numpy()\n",
    "  - parse the serialized byte string into an Example\n",
    "      - tf.Example is made of Features\n",
    "          - feature[key] == each part of the observation or data point\n",
    "          \n",
    "So, make sure this is correct.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this iteration will show you:\n",
    "# - each record\n",
    "# - each tf.Example\n",
    "# BUT - it is not a parsed tf.Example,  it looks readable, but it's not yet consumable\n",
    "#   look at the next code block for that\n",
    "\n",
    "# VERIFY your mapping using this loop\n",
    "\n",
    "for raw_record in raw_dataset.take(1):\n",
    "    print(\"raw record type:\", type(raw_record))  # serialized Example\n",
    "    print(\"Tensor.dtype:\", raw_record.dtype)\n",
    "    print(\"       value:\", raw_record.numpy()[:50], '\\n')\n",
    "    \n",
    "    example = tf.train.Example()\n",
    "    example.ParseFromString(raw_record.numpy())  # Parse will de-serialize it\n",
    "    # review this to verify the features were mapped correctly\n",
    "    print(type(example), '\\n', example)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing each tf.Example record\n",
    "\n",
    "This parses the serialized tf.Example into the feature (dict).   This is where we use that common feature definition to make sure the format is good.   feature_obj_detect is imported from:\n",
    "code.cfa_utils.example_utils.py  \n",
    "\n",
    "This isn't something I defined - I lifted it out of the code in tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"feature_obj_detect:\", type(feature_obj_detect), '\\n', feature_obj_detect)\n",
    "def _parse_function(example_proto):\n",
    "    # Parse the input using the standard dictionary\n",
    "    feature = tf.io.parse_single_example(example_proto, feature_obj_detect)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_dataset = raw_dataset.map(_parse_function)\n",
    "print (parsed_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow/models/object_detection/visualization_util.py\n",
    "\n",
    "THIS is the way to display - much easier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow/models/object_detection\n",
    "\n",
    "for n,i in enumerate(parsed_dataset.take(10)):\n",
    "    print (\"record type:\", type(i))\n",
    "    print (\"image/encoded type:\", type(i['image/encoded']))\n",
    "    image_tensor = i['image/encoded'].numpy()  # bytes\n",
    "    print (\"image/encoded EagerTensor.numpy():\", type(image_tensor))\n",
    "    print(\"is jpeg:\", tf.io.is_jpeg(image_tensor))\n",
    "    \n",
    "    jpeg_decoded_tensor = tf.image.decode_jpeg(image_tensor)\n",
    "    jpeg_numpy = jpeg_decoded_tensor.numpy()\n",
    "    print (\"tf.image.decode_jpeg(image_tensor):\", jpeg_numpy.shape)\n",
    "    \n",
    "    # get height/width\n",
    "    height = i['image/height'].numpy()\n",
    "    width =  i['image/width'].numpy()\n",
    "    \n",
    "    # get object classes\n",
    "    obj_class_names = i['image/object/class/text'].values.numpy()\n",
    "    obj_class_ids = i['image/object/class/label'].values.numpy()\n",
    "    obj_count = len(obj_class_ids)\n",
    "    \n",
    "    print (type(obj_class_names), obj_class_names)\n",
    "    # get the bounding box coordinates\n",
    "    xmins = i['image/object/bbox/xmin'].values.numpy()\n",
    "    xmaxs = i['image/object/bbox/xmax'].values.numpy()\n",
    "    ymins = i['image/object/bbox/ymin'].values.numpy()\n",
    "    ymaxs = i['image/object/bbox/ymax'].values.numpy()\n",
    "    print ('xmins:', type(xmins), xmins)\n",
    "    xmins_pixel = xmins * width\n",
    "    xmaxs_pixel = xmaxs * width\n",
    "    ymins_pixel = ymins * height\n",
    "    ymaxs_pixel = ymaxs * height\n",
    "   \n",
    "    pil_image = Image.fromarray(jpeg_numpy)    \n",
    "    for idx in range(obj_count):\n",
    "        draw_bounding_box_on_image(pil_image,ymins[idx],xmins[idx], ymaxs[idx], xmaxs[idx],\n",
    "                                  color=STANDARD_COLORS[obj_class_ids[idx]], \n",
    "                                  thickness=4, display_str_list=[obj_class_names[idx]],\n",
    "                                  use_normalized_coordinates=True)\n",
    "        \n",
    "    display.display(pil_image)\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup and Move to Training Locations\n",
    "\n",
    "!!! Change DATA_DATE !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_backup = os.path.join(PROJECT, DATA_DATE + \"_tfrecords\")\n",
    "tfrecord_source = TFRECORD_DIR\n",
    "! cd {PROJECT}\n",
    "! mkdir {tfrecord_backup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp {tfrecord_source}/train/train.* {tfrecord_backup}\n",
    "! cp {tfrecord_source}/val/val.* {tfrecord_backup}\n",
    "! cp {tfrecord_source}/test/test.* {tfrecord_backup}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tarball_name = DATA_DATE + \"_tfrecords.tar.gz\"\n",
    "# ! tar czvf $tarball_name $tfrecord_backup\n",
    "! tar cf - $tfrecord_backup | pigz > $tarball_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup tfrecords\n",
    "! aws s3 cp $tarball_name s3://jmduff.security-system/tfrecords/ --profile=jmduff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIONAL Move to ssd-dag for additional  training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- this should be deleted --\n",
    "# ssd-dag should get this from hsdata now\n",
    "os.chdir(tfrecord_backup)\n",
    "! cp train.* $SSD_TFRECORDS/train/\n",
    "! cp val.* $SSD_TFRECORDS/val/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar cf - /hsdata/annotation/202001 | pigz > annotation_202001.tar.gz\n",
    "! tar cf - /hsdata/annotation/202002 | pigz > annotation_202002.tar.gz\n",
    "! tar cf - /hsdata/annotation/202003 | pigz > annotation_202003.tar.gz\n",
    "! tar cf - /hsdata/annotation/202004 | pigz > annotation_202004.tar.gz\n",
    "! tar cf - /hsdata/annotation/202005 | pigz > annotation_202005.tar.gz\n",
    "! tar cf - /hsdata/annotation/202006 | pigz > annotation_202006.tar.gz\n",
    "! tar cf - /hsdata/annotation/202007 | pigz > annotation_202007.tar.gz\n",
    "! tar cf - /hsdata/annotation/202008 | pigz > annotation_202008.tar.gz\n",
    "! tar cf - /hsdata/annotation/202009 | pigz > annotation_202009.tar.gz\n",
    "! tar cf - /hsdata/annotation/202010 | pigz > annotation_202010.tar.gz\n",
    "! tar cf - /hsdata/annotation/202011 | pigz > annotation_202011.tar.gz\n",
    "! tar cf - /hsdata/annotation/202012 | pigz > annotation_202012.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar cf - /hsdata/jpeg_images/202001 | pigz > jpeg_images_202001.tar.gz\n",
    "! tar cf - /hsdata/jpeg_images/202002 | pigz > jpeg_images_202002.tar.gz\n",
    "! tar cf - /hsdata/jpeg_images/202003 | pigz > jpeg_images_202003.tar.gz\n",
    "! tar cf - /hsdata/jpeg_images/202004 | pigz > jpeg_images_202004.tar.gz\n",
    "! tar cf - /hsdata/jpeg_images/202005 | pigz > jpeg_images_202005.tar.gz\n",
    "! tar cf - /hsdata/jpeg_images/202006 | pigz > jpeg_images_202006.tar.gz\n",
    "! tar cf - /hsdata/jpeg_images/202007 | pigz > jpeg_images_202007.tar.gz\n",
    "! tar cf - /hsdata/jpeg_images/202008 | pigz > jpeg_images_202008.tar.gz\n",
    "! tar cf - /hsdata/jpeg_images/202009 | pigz > jpeg_images_202009.tar.gz\n",
    "! tar cf - /hsdata/jpeg_images/202010 | pigz > jpeg_images_202010.tar.gz\n",
    "! tar cf - /hsdata/jpeg_images/202011 | pigz > jpeg_images_202011.tar.gz\n",
    "! tar cf - /hsdata/jpeg_images/202012 | pigz > jpeg_images_202012.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp ./ s3://jmduff.security-system/sharded_training_data/ --profile=jmduff --recursive --exclude=\"*\" --include=\"annotation_2020??.tar.gz\"\n",
    "! aws s3 cp ./ s3://jmduff.security-system/sharded_training_data/ --profile=jmduff --recursive --exclude=\"*\" --include=\"jpeg_images_2020??.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "security",
   "language": "python",
   "name": "security"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
