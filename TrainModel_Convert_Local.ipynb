{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (after) Train Model - Convert to frozen graph & tflite\n",
    "## XPS 8100\n",
    "#### tf115_p36 environment\n",
    "\n",
    "This is designed for taking the model.pb checkpoint and converting to:\n",
    "- frozen graph  \n",
    "- tflite\n",
    "in an environment compatible with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\r\n",
      "Built on Fri_Feb__8_19:08:17_PST_2019\r\n",
      "Cuda compilation tools, release 10.1, V10.1.105\r\n"
     ]
    }
   ],
   "source": [
    "# currently CUDA 10.0\n",
    "! nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.2\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nvidia-smi\n",
    "this will show you how much memory is available in the GPU.   This is important if you start getting OOM (out of memory) errors.\n",
    "\n",
    "SageMaker p2.xlarge == 10+ GB  \n",
    "Note what is available.\n",
    "\n",
    "you can run (at a terminal)    \n",
    "  $ nvidia-smi -l 1   \n",
    "to see the GPU being used during training.  On SageMaker, you'll see the GPU is about 50% busy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr  3 10:21:07 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.64       Driver Version: 440.64       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce GTX 105...  Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 30%   40C    P0    N/A /  75W |   4013MiB /  4038MiB |      1%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0      1616      G   /usr/lib/xorg/Xorg                            40MiB |\r\n",
      "|    0      1649      G   /usr/bin/gnome-shell                          48MiB |\r\n",
      "|    0      1666      C   python                                      3537MiB |\r\n",
      "|    0      3041      G   /usr/lib/firefox/firefox                       3MiB |\r\n",
      "|    0      3661      G   /usr/lib/xorg/Xorg                           250MiB |\r\n",
      "|    0      3795      G   /usr/bin/gnome-shell                         116MiB |\r\n",
      "|    0     25999      G   /usr/lib/firefox/firefox                       1MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi\n",
    "# note memory\n",
    "# - i.e. can't run inferences while you do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your GPU\n",
    "this should verify your GPU is correct\n",
    "\n",
    "## WARNING\n",
    "this is a good test but...  \n",
    "If you run it, it may not release  the GPU memory.   I didn't figure this out fully.   When I ran it, I would get an OOM error when the model started the training cycle - even with super small batch size.   So, something is up here.   You could play around and try stopping the notebook - check nvidia-smi to verify it released the GPU RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print (sess.run(c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNet Model\n",
    "Why use a MobileNet Model?  Because the end objective is a lightweight model - one that will run on a Googl Coral TPU.    This requires a quantized model (int8 - not float32).  And, you get there from a TensorFlow Lite model.  The recommended path is to start with a model structure that you know is compatible (MobileNet) then retrain on top of it.  \n",
    "1. We pull the MobileNet v1 (there is a v2 that we aren't using) trained on COCO images\n",
    "2. We train on top of it (xfer learning) with our CFA Products\n",
    "3. That generates a TensorFlow Lite model (.tflite)\n",
    "4. We will later conver .tflite to an edge TPU model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Security - camera-api\n",
    "# 8100\n",
    "\n",
    "S3_MODEL_PATH = \"s3://jmduff.security-system/model/\"\n",
    "# base model - starting point that we train on top of\n",
    "# BASE_MODEL_FOLDER = \"20180718_coco14_mobilenet_v1_ssd300_quantized\"\n",
    "\n",
    "# project directories\n",
    "PROJECT = os.getcwd()\n",
    "\n",
    "TASKS = os.path.join(PROJECT, \"tasks\")\n",
    "MODEL_OUTPUT = os.path.join(PROJECT, 'model')\n",
    "MODEL_DOWNLOAD = os.path.join(PROJECT, \"trained_model_artifacts\")\n",
    "\n",
    "MODEL_DATE = '20200402'\n",
    "\n",
    "# Link to Security Project\n",
    "CAMERA_API = os.path.abspath(os.path.join(PROJECT, '..', 'camera-api'))\n",
    "CAMERA_API_MODEL = os.path.join(CAMERA_API, 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get (ssd-dag) Trained Model from S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://jmduff.security-system/model/20200402/checkpoint to trained_model_artifacts/checkpoint\n",
      "download: s3://jmduff.security-system/model/20200402/model.ckpt-170000.index to trained_model_artifacts/model.ckpt-170000.index\n",
      "download: s3://jmduff.security-system/model/20200402/model.ckpt.index to trained_model_artifacts/model.ckpt.index\n",
      "download: s3://jmduff.security-system/model/20200402/model.ckpt.meta to trained_model_artifacts/model.ckpt.meta\n",
      "download: s3://jmduff.security-system/model/20200402/pipeline.config to trained_model_artifacts/pipeline.config\n",
      "download: s3://jmduff.security-system/model/20200402/output_tflite_graph.tflite to trained_model_artifacts/output_tflite_graph.tflite\n",
      "download: s3://jmduff.security-system/model/20200402/model.ckpt-170000.meta to trained_model_artifacts/model.ckpt-170000.meta\n",
      "download: s3://jmduff.security-system/model/20200402/model.ckpt.data-00000-of-00001 to trained_model_artifacts/model.ckpt.data-00000-of-00001\n",
      "download: s3://jmduff.security-system/model/20200402/frozen_inference_graph.pb to trained_model_artifacts/frozen_inference_graph.pb\n",
      "download: s3://jmduff.security-system/model/20200402/saved_model/saved_model.pb to trained_model_artifacts/saved_model/saved_model.pb\n",
      "download: s3://jmduff.security-system/model/20200402/tflite_graph.pbtxt to trained_model_artifacts/tflite_graph.pbtxt\n",
      "download: s3://jmduff.security-system/model/20200402/tflite_graph.pb to trained_model_artifacts/tflite_graph.pb\n",
      "download: s3://jmduff.security-system/model/20200402/model.ckpt-170000.data-00000-of-00001 to trained_model_artifacts/model.ckpt-170000.data-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "! rm -f {MODEL_DOWNLOAD}/*.*\n",
    "! aws s3 cp {S3_MODEL_PATH}{MODEL_DATE}/ {MODEL_DOWNLOAD} --exclude='*.*' --include='*.*' --recursive --profile=jmduff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained Model Output -- IMPORTANT\n",
    "Where did it go? - THERE IS A BIG DIFFERENCE BETWEEN LOCAL TRAIN AND HOSTED TRAIN -- important !!\n",
    "\n",
    "train*.py will put the output in code/model    This is true for local or SageMaker hosted trained.   In this case, you trained locally, so the output is in code/model  -- end of story.\n",
    "\n",
    "\n",
    "When you train with a SageMaker Hosted train, the output still goes to code/model -- HOWEVER - that is in a docker image (that you will never see).  Then it gets coped to S3.   Then the notebook (TrainModel_Step3_TrainingJob) pulls a model output from S3.   Then extracts the tarball to {PROJECT}/trained_model   SO AT THIS POINT THE OUTPUT IS IN A DIFFERENT LOCATION !!\n",
    "\n",
    "The convert graph script is pulling from {PROJECT}/trained_model (not the native code/model location).    The easiest solution (you will see below) is to copy the desired checkpoint graph to the {PROJECT}/trained_model location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 288048\r\n",
      "drwxr-xr-x  3 jay jay      4096 Apr  3 10:26 .\r\n",
      "drwxr-xr-x 14 jay jay      4096 Apr  3 10:26 ..\r\n",
      "-rw-r--r--  1 jay jay        77 Apr  3 09:40 checkpoint\r\n",
      "-rw-r--r--  1 jay jay  29536515 Apr  3 09:40 frozen_inference_graph.pb\r\n",
      "-rw-r--r--  1 jay jay 109220320 Apr  3 09:40 model.ckpt-170000.data-00000-of-00001\r\n",
      "-rw-r--r--  1 jay jay     42388 Apr  3 09:40 model.ckpt-170000.index\r\n",
      "-rw-r--r--  1 jay jay  11279875 Apr  3 09:40 model.ckpt-170000.meta\r\n",
      "-rw-r--r--  1 jay jay  27381492 Apr  3 09:40 model.ckpt.data-00000-of-00001\r\n",
      "-rw-r--r--  1 jay jay     14948 Apr  3 09:40 model.ckpt.index\r\n",
      "-rw-r--r--  1 jay jay   3500465 Apr  3 09:40 model.ckpt.meta\r\n",
      "-rw-r--r--  1 jay jay   6898968 Apr  3 09:40 output_tflite_graph.tflite\r\n",
      "-rw-r--r--  1 jay jay      5103 Apr  3 09:40 pipeline.config\r\n",
      "drwxr-xr-x  2 jay jay      4096 Apr  3 10:26 saved_model\r\n",
      "-rw-r--r--  1 jay jay  27693983 Apr  3 09:40 tflite_graph.pb\r\n",
      "-rw-r--r--  1 jay jay  79346065 Apr  3 09:40 tflite_graph.pbtxt\r\n"
     ]
    }
   ],
   "source": [
    "! ls -la  {MODEL_DOWNLOAD}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "1. if you run for 500 steps, then rerun the exact process, it is going to restore /ckpt/checkpoints (ckpt-500) and then thinks it is done.  So, basically does nothing\n",
    "2. Don't delete ckpt/  (rm ckpt/*.*) WITHOUT removing ckpt/checkpoints/   The program is always checking that checkpoints subdirectory and trying to restore.  For exampmle, you delete ckpt/ but leave ckpt/checkpoints, it finds a reference to ckpt-500 but you just deleted it - so it aborts\n",
    "3. Always check your files & paths carefully - the error messages that get thrown with a missing file are not always clear - and my send you on a wild goose chase when in reality - it was just a missing file\n",
    "4. can't import nets - this is a PATH problem (models/research/slim needs to be in your path) - in the train.py program, it's programmatically added\n",
    "5. OOM when allocating tensor of shape [32,19,19,512] and type float\n",
    "\t [[{{node gradients/zeros_97}}]] -- go to the config file and change batch size to be smaller (e.g. 16)\n",
    "6. AttributeError: 'ParallelInterleaveDataset' object has no attribute '_flat_structure --- check your directories, like something didn't get installed correction (base model?  models/research stuff?  training data) -- seems to be a problem with the TF build from scratch;   use a pip install and this went away\n",
    "7. if you are mixing local ops and Docker runs - you may have messed up the ownership file outputs and checkpoints - try deleting everything and a new pull\n",
    "8. trains - then error:  TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a useable model\n",
    "At this point you have checkpoint files.   You need models (graphs).   There are many flavors:\n",
    "    - saved graph\n",
    "    - frozen graph\n",
    "    - TensorFlow Lite\n",
    "    - TensorRT\n",
    "    - EdgeTPU\n",
    "    \n",
    "The notebook:  TrainingJob_Step3_TrainingJob will show you how to convert a checkpoint file to a graph (frozen graph & tflite).   There is a bash file to do this.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 153324\n",
      "-rw-rw-r-- 1 jay jay  29536515 Mar 29 13:16 frozen_inference_graph.pb\n",
      "-rw-rw-r-- 1 jay jay 109220320 Mar 27 09:51 model.ckpt-70000.data-00000-of-00001\n",
      "-rw-rw-r-- 1 jay jay     42388 Mar 27 09:51 model.ckpt-70000.index\n",
      "-rw-rw-r-- 1 jay jay  11279875 Mar 27 09:52 model.ckpt-70000.meta\n",
      "-rw-r--r-- 1 jay jay      5056 Mar 26 14:09 mscoco_label_map.pbtxt\n",
      "-rw-r--r-- 1 jay jay   6898968 Mar 29 13:15 output_tflite_graph.tflite\n",
      "total 153324\n",
      "-rw-rw-r-- 1 jay jay  29536515 Apr  3 10:30 frozen_inference_graph.pb\n",
      "-rw-rw-r-- 1 jay jay 109220320 Mar 27 09:51 model.ckpt-70000.data-00000-of-00001\n",
      "-rw-rw-r-- 1 jay jay     42388 Mar 27 09:51 model.ckpt-70000.index\n",
      "-rw-rw-r-- 1 jay jay  11279875 Mar 27 09:52 model.ckpt-70000.meta\n",
      "-rw-r--r-- 1 jay jay      5056 Mar 26 14:09 mscoco_label_map.pbtxt\n",
      "-rw-r--r-- 1 jay jay   6898968 Mar 29 13:15 output_tflite_graph.tflite\n"
     ]
    }
   ],
   "source": [
    "# try a copy of what you already converted\n",
    "! ls {MODEL_OUTPUT} -l\n",
    "! cp {MODEL_DOWNLOAD}/frozen_inference_graph.pb {MODEL_OUTPUT}\n",
    "! ls {MODEL_OUTPUT} -l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WAKE UP - make sure NUM_TRAINING_STEPS = the max number in the checkpoint files you listed above\n",
    "#  e.g. \n",
    "# ls model\n",
    "# -rw-rw-r--  1 ec2-user ec2-user 41116528 Jan 28 15:16 model.ckpt-6000.data-00000-of-00001\n",
    "# -rw-rw-r--  1 ec2-user ec2-user    27275 Jan 28 15:16 model.ckpt-6000.index\n",
    "# -rw-rw-r--  1 ec2-user {ec2-user  6987305 Jan 28 15:16 model.ckpt-6000.meta\n",
    "NUM_TRAINING_STEPS = 170000\n",
    "! cp {CODE}/model/*{NUM_TRAINING_STEPS}* {PROJECT}/trained_model\n",
    "! ls {PROJECT}/trained_model/*{NUM_TRAINING_STEPS}*\n",
    "\n",
    "# get the config from the train*.py parameters above\n",
    "PIPELINE_CONFIG = 'local_mobilenet_v1_ssd_security_retrain.config'\n",
    "# PIPELINE_CONFIG = 'local_mobilenet_v1_ssd_retrain.config'\n",
    "! ls {CODE}/{PIPELINE_CONFIG}\n",
    "\n",
    "# if you don't see your checkpoint in */trained_model/  STOP - and fix it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert checkpoint is a task script - located in the tasks/ directory\n",
    "os.chdir(TASKS)  \n",
    "! ./convert_checkpoint_to_edgetpu_tflite.sh --checkpoint_num {NUM_TRAINING_STEPS} --pipeline_config {PIPELINE_CONFIG}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow FROZEN GRAPH\n",
    "! ls {PROJECT}/tensorflow_model -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow Lite model\n",
    "! ls {PROJECT}/tflite_model -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Security\n",
    "If you are working on the security project,   you need to:  \n",
    "put thye output_tflight_graph.tflite file in:  camera-api/model/  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the tflite model over to camera-api/model\n",
    "! cp  {PROJECT}/tflite_model/output_tflite_graph.tflite {CAMERA_API_MODEL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just checking ...\n",
    "! ls -ls {CODE}/ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the (converted?  frozen?) ckpt to the starting point\n",
    "# NOW you can re-train on top of it\n",
    "! cp {PROJECT}/tensorflow_model/model.ckpt.* {CODE}/ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup\n",
    "! aws s3 ls --profile=jmduff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DATE = '20200326'\n",
    "! aws s3 cp {PROJECT}/tensorflow_model s3://jmduff.security-system/model/{MODEL_DATE}/ --exclude='*.*' --include='*.*' --recursive --profile=jmduff\n",
    "! aws s3 cp {PROJECT}/tflite_model s3://jmduff.security-system/model/{MODEL_DATE}/ --exclude='*.*' --include='*.*' --recursive --profile=jmduff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/media/home/jay/projects/ssd-dag')\n",
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "security",
   "language": "python",
   "name": "security"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
